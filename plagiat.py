# -*- coding: utf-8 -*-
"""
plagiat.py

This script will compare all input documents against each other
and report pairs that are unusually similar. Alternatively, it
can provide a somewhat more detailed analysis of the similarities
of two given documents.

Adam Roussel
Department of Linguistics
Ruhr-University Bochum
roussel@linguistics.rub.de

"""

import math
import re
import os.path
from collections import Counter
from itertools import combinations

from nltk import ngrams
from nltk.stem.snowball import GermanStemmer, EnglishStemmer
from nltk.tokenize import word_tokenize

import langid

__version__ = '2016.06.26'

SENT_PUNCT = {hash(x) for x in ".!?;:"}
"""Set of punctuation marks that are considered to end a sentence (as hashes)."""


class Sentence:
    """
    Representation of a sentence as a span of hashed stems in the parent document.

    Attributes
    ----------
    ID: str
        Unique identifier for this sentence, composed of parent document name and
        index of this sentence in parent document.
    parent: Document
        Reference to parent document.
    start: int
        Index of first token of this sentence in the parent document.
    stop: int
        Index of last token of this sentence in the parent document.
    hashes: list[int]
        Reference to relevant hashes in parent document.
    length: int
        Length of this sentence.
    """

    def __init__(self, _ind, _par, _indices):
        """
        Parameters
        ----------
        _ind: str
            The name (or unique identifier) for this sentence.
        _par: Document
            Parent document for this sentence.
        _indices: (int, int)
            Indices of the first and last words from the parent document
            that belong to this sentence.

        """
        self.ID = _ind
        self.parent = _par
        self.start, self.stop = _indices
        self.hashes = self.parent.hashes[self.start:self.stop]
        self.length = self.stop - self.start

    def __repr__(self):
        return " ".join(self.parent.tokens[self.start:self.stop])

    def __eq__(self, other):
        return self.hashes == other

    def __hash__(self):
        return hash(self.ID)


class Document:
    """
    Represents a complete document.

    Attributes
    ----------
    string: str
        Raw string contents of the file represented by this document.
    lang: str
        Language identifier, reflects classification of `langid` module.
    name: str
        Filename of source file, without extension.
    ID: str
        Alias for `name`.
    tokens: list[str]
        Tokens in document, generated by standard NLTK tokenizer.
    stems: list[str]
        Tokens after stemming.
    length: int
        Length of document.
    hashes: list[int]
        Representation of document as list of stems, hashed for efficiency.
    sents: list[Sentence]
        Sentences in this document.
    freq_dist: dict[int, int]
        Frequency distribution for this document, mapping hashes of stems to
        their frequencies.
    """

    string = str()
    _stemmer = None

    def __init__(self, filename):
        """
        Parameters
        ----------
        filename: str
            Path to the (plaintext) file for this document.

        """
        # open file with utf-8-sig to remove any BOMs
        with open(filename, "r", encoding="utf-8-sig", errors="ignore") as infile:
            self.string = clean_whitespace(infile.read())

        self.lang = langid.classify(self.string)[0]
        if self.lang == 'de':
            self._stemmer = GermanStemmer()
        elif self.lang == 'en':
            self._stemmer = EnglishStemmer()
        else:
        	print("no stemmer for '{}'".format(self.lang))
        	print("falling back to 'de'...")
        	self._stemmer = GermanStemmer()

        self.name = os.path.splitext(os.path.split(filename)[1])[0]
        self.ID = self.name
        self.tokens = word_tokenize(self.string)
        self.stems = list(map(self.stem, self.tokens))
        self.length = len(self.tokens)
        self.hashes = list(map(hash, self.stems))
        self.sents = self._get_sents()
        self.freq_dist = dict(Counter(self.hashes))

    def __repr__(self):
        return "<Document {0}...{1}>".format(self.name[:12], self.name[-5:])

    def __eq__(self, other):
        return self.hashes == other

    def __hash__(self):
        return hash(self.name)

    def stem(self, s):
        """
        Stem token `s` with appropriate stemmer.

        Notes
        -----
        Running the result through the ASCII encoder ensures that no weird
        characters end up in the final stem (such as unusual space characters,
        which one might otherwise overlook).

        Examples
        --------
        >>> d = Document("somefile.txt")
        >>> d.stem("Versicherungen")
        'versicher'

        """
        return self._stemmer.stem(s).encode('ascii', errors='ignore').decode()

    def _get_sents(self):
        """
        Determine sentences in document according to locations of sentence
        boundary punctuation.

        Assumes that abbreviations, etc., have already been approprately tokenized.

        """
        id_counter = 0
        my_sents = []
        last_start = 0
        for i, h in enumerate(self.hashes):
            if h in SENT_PUNCT and i - last_start > 1:
                id_counter += 1
                my_sents.append(Sentence("{0}_{1}".format(self.name,
                                                          id_counter),
                                         self,
                                         (last_start, i + 1)))
                last_start = i + 1
        if self.length - last_start > 0:
            id_counter += 1
            my_sents.append(Sentence("{0}_{1}".format(self.name, id_counter),
                                     self,
                                     (last_start, self.length)))
        return my_sents


def zeros(n):
    """
    Return list of requested length, prefilled with zeros.
    """
    return [0 for _ in range(n)]


def square(x):
    return x * x


def dot_product(u, v):
    n = len(u)
    if n == len(v):
        return sum(u[i] * v[i] for i in range(n))


def norm(seq):
    return math.sqrt(dot_product(seq, seq))


def mean(seq):
    return sum(seq) / len(seq)


def std_dev(seq):
    n = len(seq)
    mean_seq = mean(seq)
    return math.sqrt(sum((seq[i] - mean_seq)**2 for i in range(n)) / (n - 1))


def covariance(u, v):
    n = len(u)
    mean_u = mean(u)
    mean_v = mean(v)
    return sum((u[i] - mean_u) * (v[i] - mean_v) for i in range(n)) / (n - 1)


def corr_coef(u, v):
    return covariance(u, v) / (std_dev(u) * std_dev(v))


def skip_bigrams(seq, w=4):
    """
    Generate set of skip-bigrams (plus unigrams) for input sequence.

    Parameters
    ----------
    seq: list
        Any list.
    w: int
        Maximum window size, i.e. how far apart the elements of a bigram are
        allowed to be.

    Returns
    -------
    set
        The resulting skip-bigrams *and* unigrams from the input sequence.

    Examples
    --------
    >>> skip_bigrams([1,2,3,4,5], w=4)
    {1, 2, 3, 4, 5, (1, 2), (1, 3), (4, 5), (1, 4), (1, 5), (2, 3),
     (2, 5), (3, 4), (2, 4), (3, 5)}

    """
    sb = set(seq)
    for i in range(len(seq)):
        for j in range(i + 1, len(seq)):
            if j - i <= w:
                sb.add((seq[i], seq[j]))    
    return sb


def skipbigram_sim(seq1, seq2, b=1.0):
    """
    Calculate skip-bigram similarity for two lists.

    Parameters
    ----------
    seq1, seq2: list
        Any lists.
    b: 1.0, optional
        Weighting factor for F-measure. Default is 1.0, balancing precision
        and recall.

    Returns
    -------
    float
        Similarity between `seq1` and `seq2` on a scale from 0 to 1.

    """
    a_set = skip_bigrams(seq1)
    b_set = skip_bigrams(seq2)
    skip2 = len(a_set & b_set)
    r = skip2 / len(a_set)
    p = skip2 / len(b_set)
    try:
        return ((1 + b**2) * r * p) / (r + b**2 * p)
    except ZeroDivisionError:
        return 0.0


def weight_by_tfidf(doc_list):
    """
    Re-weights frequencies for all documents in given list according to TF/IDF.

    Parameters
    ----------
    doc_list: list[Document]
        A list of `Document` objects.

    Returns
    -------
    None

    Notes
    -----
    Mutates documents in list!

    """
    doclen = len(doc_list)
    for doc in doc_list:
        for k, v in doc.freq_dist.items():
            doc.freq_dist[k] = int(bool(v)) * math.log2(doclen /
                                                        len(list(filter(lambda x: k in x.freq_dist,
                                                                        doc_list))))


def doc_distance(d1, d2, compare):
    """
    Calculate distance between two documents.

    Parameters
    ----------
    d1, d2: Document
        Any `Document` object.
    compare: function
        Any function which can calculate the distance between two numpy vectors.

    Returns
    -------
    float
        Distance between the two documents (usually, depends on return type of
        given function).

    """
    dims = list(set(d1.hashes + d2.hashes))
    N = len(dims)
    v1 = zeros(N)
    v2 = zeros(N)
    d1freqs = d1.freq_dist
    d2freqs = d2.freq_dist
    for i in range(N):
        current_dim = dims[i]
        v1[i] += d1freqs.get(current_dim, 0)
        v2[i] += d2freqs.get(current_dim, 0)
    return compare(v1, v2)


def sent_to_chars(s, join_char=''):
    """
    Convert a sentence to a sequence of characters.

    Parameters
    ----------
    s: Sentence
        Some input Sentence object.
    join_char: '', optional
        Characters to be inserted between tokens. If empty string, resulting
        character n-grams will ignore spaces, or one can use ' ' to include
        spaces in character n-grams.

    Returns
    -------
    str
        
    Notes
    -----
    Intended for use with character n-grams.

    Examples
    --------
    >>> sent_to_chars(some_sentence_obj)
    'thisisanexamplesentence'

    """
    return (join_char.join(s.parent.tokens[s.start:s.stop])).lower()


def cosine_sim(x, y):
    """
    Calculate cosine similarity between two vectors.

    Parameters
    ----------
    x, y: list
        A list of numbers.

    Returns
    -------
    float
        The cosine similarity (0 = orthogonal, 1 = identical).

    """
    norm_x = norm(x)
    norm_y = norm(y)
    return dot_product(x, y) / (norm_x * norm_y)

    
def jaccard_sim(seq1, seq2, n=4):
    """
    Compute Jaccard index for two lists.

    Parameters
    ----------
    seq1, seq2: list
        Two lists to be compared.
    n: 4, optional
        Size of n-grams to use in the comparison.

    Returns
    -------
    float
        The similarity between items in lists, according to Jaccard index.

    """
    n1 = nltk_ngrams(seq1, n)
    n2 = nltk_ngrams(seq2, n)
    return len(set(n1).intersection(set(n2))) / len(set(n1).union(set(n2)))


def nltk_ngrams(seq, size):
    """
    Get n-grams for input sequence.

    Parameters
    ----------
    seq: list
        Any list.
    size: int
        Size of desired n-grams.

    Returns
    -------
    list[tuple]
        List of tuples of size `size` (or possibly smaller, if input 
        sequence is too short.)
    
    Notes
    -----
    Wraps the NLTK n-grams function to provide lists as the return type
    and to alter the function's behavior in the case where the input
    sequence is shorter than the desired n-grams.

    """
    if len(seq) >= size:
        return list(ngrams(seq, size))
    else:
        return [tuple(seq)]


def score_all_sentpairs(d1, d2):
    """
    Compare all sentences between two documents.

    Parameters
    ----------
    d1, d2: Document
        Documents to be compared.
    
    Returns
    -------
    dict[(Document, Document), float]
        Maps pairs to Jaccard distances for all compared sentence pairs 
        between the two input documents.

    """
    return {(i, j): jaccard_sim(sent_to_chars(i),
                                sent_to_chars(j))
            for j in d2.sents
            for i in d1.sents}


def get_alignments(sent_pair_scores, threshold=2.0):
    """
    Filter dictionary of all pairs down to just unusually similar pairs.

    Parameters
    ----------
    sent_pair_scores: dict[(Document, Document), float]
        Dictionary mapping document pairs to the measured distance between them.
    threshold: 2.0, optional
        Number of sigma (std. dev.) above the avg. document similarity a pair
        must be to be considered suspicious.

    Returns
    -------
    dict[(Document, Document), float]
        Dictionary from pair to score for all suspicious pairs.

    """
    all_scores = [y for _, y in sent_pair_scores.items()]
    m = mean(all_scores)
    s = std_dev(all_scores)

    # matches when *distance* measure is unusually low
    return {k: v for k, v in sent_pair_scores.items()
            if v > m + threshold * s}


def write_sentpairs_to_gexf(matchlist, filename):
    """
    Export a list of compared pairs as graph edges (in GEXF format), 
    weighted by their computed distance from one another.

    Parameters
    ----------
    matchlist: dict[(obj, obj), float]
        Dictionary from `Sentence` or `Document` pairs to the distance
        between the elements of the pair.
    filename: str
        Location for the GEXF-format exported data.

    Returns
    -------
    None

    """
    import networkx as nx
    g = nx.Graph()
    for (pairL, pairR), score in matchlist.items():
        g.add_edge(pairL.ID, pairR.ID, weight=score)
    nx.write_gexf(g, filename)


def clean_whitespace(a_string):
    """Convert any whitespace characters in string to spaces."""
    return re.sub(r"[\n\r\t]", " ", a_string)


def output_matching_docpair(doc1, doc2):
    """
    Provide detailed information comparing two documents.

    Prints avg. Jaccard distance between all sentences, Pearson's 'r'
    correlation between indices of matching sentences (whether matching
    sentences occur in the same order), and the percentage of sentences 
    that matched significantly with some sentence in the other document.

    Parameters
    ----------
    doc1, doc2: Document
        Two documents to be compared.
    
    Returns
    -------
    None

    """
    doc1_numsents = len(doc1.sents)
    doc2_numsents = len(doc2.sents)

    big_n = max(doc1_numsents, doc2_numsents)

    lind = []
    rind = []
    output_format = "---\n{num}\n--\n{a_ID}: {a}\n--\n{b_ID}: {b}\n---"

    all_pairs = score_all_sentpairs(doc1, doc2)
    alignments = get_alignments(all_pairs)
    for (pairL, pairR), score in alignments.items():
        print(output_format.format(num=score,
                                   a_ID=pairL.ID, a=pairL,
                                   b_ID=pairR.ID, b=pairR))

        lind.append(int(pairL.ID.split("_")[-1]))
        rind.append(int(pairR.ID.split("_")[-1]))

    print("Avg. match score:", mean([score for _, score in alignments.items()]))

    print("Pearson's r:", corr_coef(lind, rind))

    print("Matched sentences:", len(alignments) / big_n, "%")

    
def get_args(args):
    """Parse command line arguments."""
    import argparse
    myparser = argparse.ArgumentParser(
        description='Identify suspiciously similar documents.')
    myparser.add_argument('filenames', type=str, nargs='+',
                          help='List of files to analyze.')
    myparser.add_argument('-p', '--pair', action='store_true',
                          help='Provide detailed comparison of just two files.')
    myparser.add_argument('-l', '--lang', type=str, default='de',
                          help='Language of files to be analyzed. (default: "de")')
    myparser.add_argument('-t', '--threshold', type=float, default=2.0,
                          help='Threshold for "unusually similar" pair, in sigma. (default: 2.0)')
    myparser.add_argument('--version', action='version', version=__version__)
    return myparser.parse_args(args)


def main():
    """
    The main function will
    1. Interpret user args.
    2. If 'pair' active, print detailed comparison of two files.
    3. Else, compare all input documents in selected 'lang'.
    4. And print list of those with especially low distance score 
        between them.
    Then exit.

    """
    
    import sys
    myargs = get_args(sys.argv[1:])
    
    if myargs.pair:
        doc1, doc2, *rest = myargs.filenames
        output_matching_docpair(Document(doc1), Document(doc2))
    else:
        all_docs = [Document(d) for d in myargs.filenames]
        docs_lang_filtered = [x for x in all_docs if x.lang == myargs.lang]
        weight_by_tfidf(docs_lang_filtered)

        doc_pair_scores = {(d1, d2): doc_distance(d1, d2, cosine_sim)
                           for d1, d2 in combinations(docs_lang_filtered, 2)}

        doc_pair_matches = get_alignments(doc_pair_scores, 
                                          threshold=myargs.threshold)
        
        for x in sorted(doc_pair_matches, key=doc_pair_matches.get, reverse=True):
            print("{pair}\t{score:.3f}".format(pair=x, 
                                               score=doc_pair_matches[x]))
            
        print(len(doc_pair_matches), "matches")

    sys.exit(0)
    

if __name__ == '__main__': main()

